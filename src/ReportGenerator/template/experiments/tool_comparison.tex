\section{GPURepair vs. AutoSync Comparison}

\subsection{Result Comparison}
\tableref{results} summarizes the results obtained from running the benchmark suite with \tool and \autosync. The table categorizes the results into three categories based on the output of \verifier. Since \autosync can not take OpenCL kernels as inputs, results for OpenCL is only applicable for \tool.

For $@@autosync_repair_error@@$ benchmarks, \autosync stated that the error could not be repaired. Out of these $@@autosync_repair_error@@$, \tool was able to repair $@@autosync_gr_repaired@@$, it timed out for $@@autosync_gr_timeout@@$, and it could not repair $@@autosync_gr_others@@$ of these.

\input{tables/results}

\subsection{Time Comparison}
As described in \tableref{results}, there were $@@unrepairable_errors@@$ benchmarks in the third category where \verifier itself throws an irreparable error (e.g., either an assertion violation or errors thrown by other stages). We provide a time comparison via the scatter plot shown in \figref{time_scatter_log} for the remaining $@@valid_count@@$ benchmarks which are either certified as error free by \verifier or having a presence of data race and/or barrier divergence error.

Out of the $@@valid_count@@$ benchmarks, \autosync was faster for $@@autosync_faster@@$ whereas \tool was faster for $@@gpurepair_faster@@$ benchmarks. Please note that out of these $@@valid_count@@$ benchmarks, there are $@@autosync_noeffort@@$ benchmarks for which \verifier does not show any error. For these benchmarks, \autosync does not have to put any further efforts. On the other hand, \tool attempts to find if there are some programmer inserted barriers that are unnecessary so that they can be removed. \figref{time_repair_log} shows run time comparision for $@@repairable_errors@@$ benchmarks for which \verifier found data-race/barrier divergence errors. \autosync was faster on $@@autosync_repairable_faster@@$ benchmarks whereas \tool was faster on $@@gpurepair_repairable_faster@@$ benchmarks out of these $@@repairable_errors@@$ benchmarks. Note that if any of the tools crash on a benchmark, we consider that run to have timed out (e.g., $300$ seconds).

The total time taken for all $@@valid_count@@$ benchmarks on average by \autosync is $@@autosync_time@@$ seconds whereas \tool takes $@@gpurepair_time@@$ seconds. The median time taken over all the $@@valid_count@@$ benchmarks by \autosync is $@@autosync_median@@$ seconds whereas the median time taken by \tool is $@@gpurepair_median@@$ seconds.

\input{figures/time_total}

\subsection{Verifier Calls Comparison}
There were $@@repaired_count@@$ CUDA kernels that were repaired by both \tool and \autosync. For $@@unchanged_count@@$ CUDA kernels neither \tool nor \autosync suggest any changes. \figref{verifier_calls_all} shows the number of queries made to \verifier by \autosync v/s \tool for all $@@repaired_count@@+@@unchanged_count@@=@@repaired_unchanged@@$ kernels which both of them repaired or both of them concurred that no changes are required. The size of the dot is proportional to the number of instances. In total for these $@@repaired_unchanged@@$ benchmarks, \autosync invoked \verifier $@@autosync_both_vercalls@@$ times whereas \tool queried \verifier $@@gpurepair_both_vercalls@@$ times.

\figref{verifier_calls_fixes} provides a comparison to the calls made to the \verifier for only $@@repaired_count@@$ benchmarks that are repaired by both \tool and \autosync. \tool makes $@@gpurepair_repaired_vercalls@@$ queries while \autosync makes $@@autosync_repaired_vercalls@@$ calls to \verifier.

\input{figures/verifier_calls}
