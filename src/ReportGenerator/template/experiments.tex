\section{Benchmark Summary}
For experiments, a total of $752$ kernels ($266$ CUDA and $486$ OpenCL) were considered. This benchmark set is a union of four independent test suites and publicly available \cite{cudatoolkit} sample programs. The source data used to generate this report is available in the $\MB{generated}$ and $\MB{analysis}$ directories. \tableref{benchmark_summary} summarizes the various sources of the kernels.

\input{tables/benchmark_summary}

\section{GPURepair vs. AutoSync Comparison}

\subsection{Result Comparison}
\tableref{results} summarizes the results obtained from running the benchmark suite with \tool and \autosync. The table categorizes the results into three categories based on the output of \verifier. Since \autosync can not take OpenCL kernels as inputs, results for OpenCL is only applicable for \tool. Numbers in bold indicate better results.

For $9$ benchmarks, \autosync stated that the error could not be repaired. Out of these $9$, \tool was able to repair $4$, it timed out for $3$, and it could not repair $2$ of these.

\input{tables/results}

\subsection{Time Comparison}
As described in \tableref{results}, there were $25$ benchmarks in the third category where \verifier itself throws an irreparable error (e.g., either an assertion violation or errors thrown by other stages). We provide a time comparison via the scatter plot shown in \figref{time_scatter_log} for the remaining $241$ benchmarks which are either certified as error free by \verifier or having a presence of data race and/or barrier divergence error.

Out of the $241$ benchmarks, \autosync was faster for $177$ whereas \tool was faster for $64$ benchmarks. Please note that out of these $241$ benchmarks, there are $152$ benchmarks for which \verifier does not show any error. For these benchmarks, \autosync does not have to put any further efforts. On the other hand, \tool attempts to find if there are some programmer inserted barriers that are unnecessary so that they can be removed. \figref{time_repair_log} shows run time comparision for $89$ benchmarks for which \verifier found data-race/barrier divergence errors. \autosync was faster on $31$ benchmarks whereas \tool was faster on $58$ benchmarks out of these $89$ benchmarks. Note that if any of the tools crash on a benchmark, we consider that run to have timed out (e.g., $300$ seconds).

The total time taken for all $241$ benchmarks on average by \autosync is $17302$ seconds whereas \tool takes $6590$ seconds. The median time taken over all the $241$ benchmarks by \autosync is $1.44$ seconds whereas the median time taken by \tool is $1.81$ seconds.

\input{figures/time_total}

\subsection{Verifier Calls Comparison}
There were $28$ CUDA kernels that were repaired by both \tool and \autosync. For $129$ CUDA kernels neither \tool nor \autosync suggest any changes. \figref{verifier_calls_all} shows the number of queries made to \verifier by \autosync v/s \tool for all $28+129=157$ kernels which both of them repaired or both of them concurred that no changes are required. The size of the dot is proportional to the number of instances. In total for these $157$ benchmarks, \autosync invoked \verifier $212$ times whereas \tool queried \verifier $294$ times.

\figref{verifier_calls_fixes} provides a comparison to the calls made to the \verifier for only $28$ benchmarks that are repaired by both \tool and \autosync. \tool makes $83$ queries while \autosync makes $137$ calls to \verifier.

\input{figures/verifier_calls}

\section{Source Code Information}

\figref{code_lines} shows the size of the kernels in terms of lines of code. The average number of lines of code for the $752$ kernels in the evaluation set was $17.67$, and the median was $11$. $14$ kernels had more than $100$ lines of code, and $47$ had more than $50$ lines of code. $50\%$ of the kernels had less than $25$ lines of code. The kernel with the highest number of lines of code had $639$ lines.

\input{figures/lines_commands}

It should be noted that the instrumentation step of \tool happens on the Boogie code and not on the source code. A line of code in the source file could result in no Boogie commands (e.g., code comments) or more than one Boogie command (e.g., multiple assignments on a single line). \figref{boogie_commands} shows the size of the kernels in terms of Boogie commands. The average number of Boogie commands for the $734$ kernels for which Boogie code was generated was $25.72$, and the median was $11$. $50\%$ of the kernels had less than $25$ commands. The kernel with the highest number of Boogie commands had $1793$ commands.

\input{figures/instrumentation}

The repair step of \tool does not depend on the number of lines or code nor the number of Boogie commands. It depends on the number of barriers that are instrumented in code which depend on the number of shared variables and how many times they have been used.

\figref{kernels_barriers} shows the number of barrier variables introduced in the instrumentation stage of \tool. This also includes the barrier variables introduced for existing barriers provided by the programmer. Out of the $734$ kernels that reached the instrumentation stage (inclusive of CUDA and OpenCL kernels), for $50\%$ of the kernels, the number of instrumented barriers introduced were less than or equal to $3$, whereas, there were $5$ kernels with more than $100$ instrumented barriers. For $50\%$ of the kernels, the time taken by the instrumentation stage of \tool was less than a second.

\section{Solver Comparison}

Besides the $mhs$ implementation of \tool, we introduce two more implementations of \tool, which use $MaxSAT$ and $SAT$ solvers, respectively, to solve the clauses. These implementations of \tool are used to repair all the $752$ kernels from \tableref{benchmark_summary}. \tableref{solver_comparison} summarizes the metrics of these three implementations. \figref{mhs_maxsat} and \figref{mhs_sat} show the time comparison between mhs and MaxSAT.

\input{figures/time_solvers}
\input{tables/solver_comparison}

All three implementations repaired $83$ kernels ($53$ CUDA and $30$ OpenCL). \tableref{solver_comparison_repaired} summarizes the metrics of these three implementations for these repaired kernels.

\input{tables/solver_comparison_repaired}

\section{Configuration Comparison}

\tool provides the option of disabling instrumentation of grid-level barriers and disabling inspection of programmer inserted barriers. Disabling these configuration have a significant performance impact. By default both these options are enabled. These configuration options are used to repair all the $752$ kernels from \tableref{benchmark_summary}. \tableref{configuration_comparison} summarizes the metrics of these four configurations.

\input{tables/configuration_comparison}

All four implementations give the same result for $83$ kernels ($53$ CUDA and $30$ OpenCL). \tableref{configuration_comparison_same} summarizes the metrics of these four configurations for these kernels.

\input{tables/configuration_comparison_same}
